# metabench - a sparse benchmark to measure general ability in LLMs
Based on item response theory analyses of over 5000 LLMs, ğš–ğšğšğšŠğš‹ğšğš—ğšŒğš‘ distills the [Open LLM Leaderboard 1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard) to less than 3% of its original size with only 0.6% mean absolute reconstruction error of the original score. This repo contains the source code for [scraping](scraping) in Python and [analysis](analysis) in R. 

[ğš–ğšğšğšŠğš‹ğšğš—ğšŒğš‘ - A Sparse Benchmark to Measure General Ability in Large Language Models](https://arxiv.org/abs/2407.12844)
