# metabench - a sparse benchmark to measure general ability in LLMs
Based on item response theory analyses of over 5000 LLMs, 𝚖𝚎𝚝𝚊𝚋𝚎𝚗𝚌𝚑 distills the [Open LLM Leaderboard 1](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard) to less than 3% of its original size with only 0.6% mean absolute reconstruction error of the original score. This repo contains the source code for [scraping](scraping) in Python and [analysis](analysis) in R. 

[𝚖𝚎𝚝𝚊𝚋𝚎𝚗𝚌𝚑 - A Sparse Benchmark to Measure General Ability in Large Language Models](https://arxiv.org/abs/2407.12844)
