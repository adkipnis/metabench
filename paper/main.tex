\documentclass{article}
\usepackage{neurips_data_2024}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{lipsum}
\usepackage{graphicx}       % include pdfs as figures
\usepackage[ruled]{algorithm2e}
\usepackage{bm}

\title{\texttt{metabench}\\A Sparse Benchmark to Measure General Ability\\in Large Language Models}
\author{%
   Alex Kipnis $^{1}$ \thanks{Correspondence to \texttt{adkipnis@mailbox.org}} \quad
   Luca M. Schulze Buschoff $^{1}$ \quad
   Konstantinos Voudouris $^{1,2}$ \quad
   Eric Schulz $^1$\\
   $^1$ Human-Centered AI, Helmholtz Munich \quad $^2$ University of Cambridge\\
}

\begin{document}

\maketitle

\begin{abstract}
   Large Language Models (LLMs) vary in their abilities on a range of tasks. Initiatives such as the \texttt{Open LLM Leaderboard} aim to quantify these differences with several large benchmarks (sets of test items to which an LLM can respond either correctly or incorrectly).
   However, high correlations within and between benchmark scores suggest that (1) there exists a small set of common underlying abilities that these benchmarks measure, and (2) the latter tap into redundant information and may thus be considerably compressed.
   We use data from over $5.000$ LLMs to analyze the psychometric properties of six benchmarks (\texttt{ARC}, \texttt{GSM8K}, \texttt{HellaSwag}, \texttt{MMLU}, \texttt{TruthfulQA} and \texttt{WinoGrande}) in order to identify their most informative items.
   Using this subset, we construct a sparse benchmark, \texttt{metabench}, that has less than $4$\% of the original size of all six benchmarks combined. Moreover, \texttt{metabench} goes beyond point scores by yielding estimators of the underlying benchmark-specific abilities.
   We show that these estimators (1) can be used to reconstruct the original \textit{individual} benchmark scores, and (2) have one underlying common factor which can be used to reconstruct the original \textit{total} score with $\sim 2$\% error.
\end{abstract}

% ------------------------------------------------------------------------------------
% Introduction
\section{Benchmarks do not need to be big}
\subsection{Related Work}

% ------------------------------------------------------------------------------------
% Methods and Results
\section{Benchmark Distillation}
\begin{figure}[h]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/overview.pdf}
   \caption{\textit{Processing pipeline}. (1) Collect item-wise accuracies from all available LLMs for each benchmark on \texttt{Open LLM Leaderboard}. (2) Remove items solved by more than $95\%$ of LLMs and items with part-whole correlation $r \approx 0$. (3) Fit variants of IRT models to the remaining items and choose the best fit by cross-validation. (4) Infer item information from the item parameters and filter out uninformative items to (5) construct \texttt{metabench}. (6) Use the item parameters to estimate the benchmark-specific abilities and use factor analysis to find a common factor. (7) These can be used to reconstruct the original (normalized) benchmark scores as well as their mean.}
   \label{fig:overview}
\end{figure}


\subsection{Data Collection and Preprocessing}
% openllm leaderboard
We collected openly available item-wise accuracies from \href{https://huggingface.co/datasets}{Hugging Face Datasets} for the six benchmarks that are part of the \href{https://huggingface.co/open-llm-leaderboard}{\texttt{Open LLM Leaderboard}}: \texttt{ARC}, \texttt{GSM8K}, \texttt{HellaSwag}, \texttt{MMLU}, \texttt{TruthfulQA} and \texttt{WinoGrande}. % TODO: add citation
After excluding flagged and merged models, we obtained data from $1355$ unique users, yielding in total $6875$ LLMs out of which $5055$ had data for all six benchmarks. 
For comparability, we normalized the benchmark scores on a percent scale and saved for future reference. Per benchmark, we removed LLMs with the lowest $0.1\%$ of scores to reduce noise.
Items with duplicate prompts, standard deviation below $1\%$ and easiness above $95\%$ were removed. We then estimated the part-whole correlation between  items and the total score using the point-biserial correlation coefficient:
\begin{equation}
   r_{pbis} = \frac{\mu_1 - \mu}{s_{n-1}} * \sqrt{\frac{n_1 * (n-n_1)}{n*(n-1)}},
\end{equation}
where $\mu_1$ resp. $\mu$ are the mean scores of the group who answered the item correctly resp. the total group, $s_{n-1}$ is the standard deviation of the total group, $n_1$ resp. $n$ are the size of the group that answered the item correctly resp. the total group. Items with $r_{pbis} = 0 \pm \epsilon$ were removed, $\epsilon$ was set to $0.05$ except for \texttt{Winogrande} where it was set to $0.02$ as otherwise hundreds of items would have been removed.\footnote{Note that it is customary to remove items with negative $r_{pbis}$. However, items with a negative $r_{pbis}$ \textit{do} contain information about the benchmark score. We therefore chose to keep them.}
For comparability, we conducted a train-test-validation split of the LLMs in the following manner: For stratification, we calculated the grand average (GA) of the original benchmark scores for models that had data for \textit{all six} benchmarks, and used it to split off $10\%$ of the LLMs as the global test set. For cross-validation per benchmark, we split off further $10\%$ of the remaining LLMs as the local validation set, this time stratifying by the specific benchmark score. 


\subsection{Evolutionary Subsampling}
After preprocessing, \texttt{HellaSwag} and \texttt{MMLU} still had substantially more items than LLMs, prohibiting the use of IRT models. To further reduce their number of items to $1/4$ of the respectively available LLMs, we used an evolutionary subsampling approach:
\begin{enumerate}
   \item Randomly drop $k$ items from the benchmark.
   \item Re-calculate the mean score $\dot {\boldsymbol{\mu}}$ of the remaining items for each LLM.
   \item Fit a generalized additive regression model (GAM) of the original mean score using the training set: $\boldsymbol\mu = g(\dot{\boldsymbol\mu})$ 
   \item Calculate its root mean square error (RMSE) on the validation subset.
\end{enumerate}
This inner loop is repeated $50$ times, and the iteration with the lowest RMSE makes the cut. The outer loop is repeated until the number of items reaches the goal size. The final set of items and and the final regression model are tested on the global test set. For \texttt{MMLU}, the procedure uses a linear model of all scenario scores to predict the total score, but is otherwise identical.
Using this procedure, we reduced the number of items in \texttt{HellaSwag} from $10042$ to $1181$ and in \texttt{MMLU} from $14042$ to $1108$ with minimal loss in predictive performance.
\begin{figure}[]
   \centering
   \includegraphics[width=0.8\textwidth]{figures/evo.pdf}
   \caption{\textit{Score reconstruction after evolutionary subsampling}. Normalized mean scores reconstructed for the test set using items selected by the evolutionary subsampling procedure (A,C) vs. a random selection of the same size (B,D).}
   \label{fig:hs-reduced}
\end{figure}


\subsection{Item Response Theory}
% IRT in a nutshell
Item response theory (IRT) is a branch of psychometrics that models the relationship between a person's ability and the probability of a correct response to an item. It is closely related to factor analysis, but assumes binary or polytomous responses. %TODO cite
Let $x_{ij}$ be the accuracy of item $i$ for subject $j$ with some (unknown) latent ability $\boldsymbol \vartheta_j$, then the probability of a correct response is modeled as:
\begin{equation}
   \mathbb P(x_{ij}=1|\boldsymbol \vartheta_j) = f(x_{ij}|\boldsymbol \vartheta_j, \boldsymbol \alpha_i),
\end{equation}
where $f$ is a link function and $\boldsymbol \alpha_i$ are the item parameters. A simple IRT model is the 2-parameter logistic model (2PL) with $\boldsymbol \alpha_i = (d_i, \mathbf a_i)$ and $f(x_{ij}|\boldsymbol \vartheta_j, \boldsymbol \alpha_i) = G(\mathbf a_i^\top  \boldsymbol \vartheta_j - d_i)$, where $G$ is the logistic function, $\mathbf a_i$ is the loading parameter and $d_i$ is the difficulty parameter.\footnote{Generally, $\mathbf a_i$ can be a vector, but parameter recovery scales poorly with the dimensionality of this vector. As higherdimensional IRT models require much larger subjects-to-items ratios than in our case, we chose to only estimate unidimensional models.} The primary goal is to estimate the item parameters $\mathbf A = (\boldsymbol \alpha_i)_{i = 1, \ldots, d}$. As the latent ability is generally also unkown, the gold-standard approach is to use marginal maximum likelihood methods, which treat the latent ability as a random variable and attempt to maximize $$\mathcal L(\mathbf A) = \prod_{i=1}^n \int \prod_{j=1}^d f(x_{ij}|\boldsymbol \vartheta_i, \boldsymbol\alpha_j) f(\boldsymbol \vartheta_i) \, \mathrm d\boldsymbol \vartheta_i.$$ Due to the integral there is no analytical solution to this problem, but it can be approximated using Expectation-Maximization (EM) algorithms. %TODO cite
After model fitting, $\boldsymbol \vartheta_j$ can be estimated for each subject $j$ using e.g. maximum a posteriori estimation. %TODO cite

% what we do
We used the \texttt{mirt} package in R to fit the following IRT models to the remaining items - a $2PL$ model, a $3PL$ model with an additional guessing parameter, a $4PL$ model with an additional upper asymptote parameter and another two $2PL$ model with a two-dimensional latent ability. Although the latter models had the fit for \texttt{GSM8K} and \texttt{MMLU}, we decided to keep the dimensionality of $\boldsymbol \vartheta$ to $1$, as this the second latent dimension was strongly negatively correlated with the scores, and the information filtering procedure described below does not extend trivially to higher dimensions.
Per benchmark, we used training set data from all available LLMs after preprocessing. Note that most users had several LLMs, which potentially violates the IRT model assumption that the response patterns of two different subjects are independent conditional on the item parameters. To assess the severity of this violation, we compared the empirical distributions of GA scores for (1) all LLMs vs. (2) all latest LLMs per user. As the distributions are nearly identical and as the ratio between LLMs and items is already low for IRT standards, we treat all LLMs as independent observations.
After model fitting, we used both maximum a posteriori as well as expected a posteriori estimation for the latent ability of each LLM. We then fit a GAM of the original benchmark score predicted by the latent ability, and test the model on the test set. Figure \ref{fig:score.full} shows the results for the best-fitting model per benchmark.
% include results about scores here
\begin{figure}[]
   \centering
   \includegraphics[width=1.0\textwidth]{figures/score.full.pdf}
   \caption{\textit{IRT scores}. Latent abilities are estimated with IRT models. They carry enough information to recover the original normalized scores at around $1\%$ RMSE per benchmark. Performance was evaluated on LLMs in the test set, unseen by both the IRT model and the nonlinear regression model.}
   \label{fig:score.full}
\end{figure}


\subsection{Information Filtering}
\begin{figure}[]
   \centering
   \includegraphics[width=0.5\textwidth]{figures/icc.pdf}
   \caption{\textit{Relation between item parameters and information}. (Top) Item characteristic curves for four items with different parameters. When not indicated, the difficulty parameter is set to $0$ and the loading parameter to $1$. (Bottom) Information of the same items as a function of the LLM's ability.}
   \label{fig:icc}
\end{figure}
% TODO add left scatter plot (difficulty vs. loading) and pick out a couple of prompts!

Since we model the probability of responding correctly to an item as a function of its parameters and the LLM's ability, the item parameters have a natural interpretation. The difficulty parameter determines the probability of a correct response for an LLM with $\vartheta_j = 0$, while the loading parameter determines how much the probability of a correct response changes with the test taker's ability. The information of an item is the expected curvature of the log-likelihood function. Concretely, for the 2PL model $$I_i(\vartheta) = a_i^2 \cdot p(x_{i}=1|\vartheta) \cdot (1 - p(x_{i}=1|\vartheta)),$$ where $p(x_{i}=1|\vartheta)$ is the probability of solving item $i$ evaluated in $\vartheta$. On a high level, this is a measure of how informative an item is about a test taker's ability \textit{conditioned on the ability} (see Figure \ref{fig:icc}). Importantly, this provides a principled approach for selecting items:
\begin{enumerate}
   \item Estimate the item parameters using an appropriate IRT model.
   \item Calculate the information function using the item parameters.
   \item Partition the empirical distribution of the latent ability into $k$ quantiles.
   \item In each quantile, select $n$ unique items with the highest information (above a certain threshold $\tau$).
\end{enumerate}
This sweep algorithm notably has three hyperparameters ($k$, $n$, $\tau$) allowing for data-driven finetuning with e.g. Bayesian optimization. Again, we use a validation set to select the best hyperparameters and test the final model on the global test set.
% reconstruction from reduced set


\subsection{Adaptive Testing}
A large benefit of IRT analysis is that it allows for adaptive testing. The information filtering procedure can be seen as a one-size fits most approach, covering as much ground as necessary to get a good estimate of the latent ability (within quantiles populated by the majority of models). However, there is also a made-to-measure alternative which can actually end up costing less: By asking the right questions sequentially, one can get a good estimate of the latent ability with fewer items. This is the idea behind adaptive testing.
[Kozzy's part]


\subsection{Going Meta}
% reconstruction of total score
% exploratory factor analysis


\section{Why Latent Abilities Matter}
% logistic reconstruction
% random subsampling


\section{Conclusion}
\subsection{Limitations}
% ratio of LLMs to items
% independence assumption

\subsection{Future Work}

% ------------------------------------------------------------------------------------
%  Appendix
\section*{Acknowledgments}
\section*{References}
\section*{Appendix}
\end{document}
